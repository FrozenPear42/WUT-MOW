---
title: "Metody odkrywania wiedzy"
author: "Rafał Galczak, Wojciech Gruszka"
output: 
  pdf_document: default
  html_notebook: default
---

```{r}
library(ggplot2)
```

#Projekt

## Zagadnienie

___Temat:___ _Lokalna regresja (predykcja wartości funkcji docelowej dla przykładu za pomocą modelu jednorazowego użytku budowanego na podstawie jego „najbliższych sąsiadów”). Eksperymenty z kilkoma różnymialgorytmami do budowania modeli lokalnych. Porównanie z algorytmami regresji dostępnymi w R._

W ramach projektu należało napisać z użyciem pakietu R biblioteki/funkcji, która pozwoli na zastosowanie algorytmów regresji dostępnych w pakiecie R w trybie "lokalnym". sprowadza się to do znalezienia w zbiorze danych (który musi być do tego odpowiednio przygotowany) zbioru trenującego dla danego przykładu z użyciem algorytmu k-najbliższych sąsiadów. Następnie dla takiego zbioru należy zastosować wybrany algorytm regresji z pakietu R. Jako algorytmy budowania modeli należy zastosować dostępne w pakiecie R algorytmy regresjii. Wykorzystane algorytmy zostały opisane w dalszej części.

W ramach projektu miały zostać przeprowadzone testy sprawdzające działanie usyskanego kodu dla kilku algorytmów oraz zbiorów danych.

## Czemu warto zająć się takim zagadnieniem
Samo zadanie jest dość ciekawe ze względu na to, że istnieją sensowne algorytmy pozwalające na realizację regresji lokalnej takie jak _LOES_ czy _LOWESS_. Jednym z powodów może być tutaj chęć przetestowania sprawności algorytmów regresji w trybie lokalnym i weryfikacji poprawności ich działania. Zastosowanie algorytmów budowanaia modeli regresji w trybie lokalnym pozwala na znaczne zmniejszenie czasu i zasobów potrzebnych do wykonania predykcji, co wynika ze znacznego ograniczenia liczebności danych trenujących do tylko tych z bezpośredniego obszaru (k-najbliższych sąsiadów).


## Zbiory danych
Do projektu musieliśmy wybrać zbiory danych testowych które powinny spełniać wymagania:
 - liczba atrybutów powinna być odpowiednio duża (>5 atrybutów)
 - zmienna objaśniana powinna być ciągła lub zachowywać sens w przypadku interpolacji (np. ocena filmu w skali dyskretnej 1-10 zachowuje sens dla wartości 9.81)
 - zbiór danych powinien zawierać odpowiednio dużo przykładów

Po długich poszukiwaniach sensownych i odpowiednio ciekawych danych do analizy na dostępnych portalach udostępniających zbiory danych (Kaggle, Data.World) zdecydowaliśmy się na następujące propozycje:

### Zbiór dancyh wypożyceń rowerów w systemie wypożyczalni
Dane pochodzą ze strony: [https://data.world/data-society/capital-bikeshare-2011-2012]().

Zbiór zawiera 17379 przykładów dotyczących wypożyczeń rowerów w systemie wypożyczalni w Waszyngtonie. Dostępne są atrybuty związane z czasem wypożyczenia (dzień tygodnia, godzina), pogodą (temperatura, wiilgotność, prędkość wiatru) oraz liczby wypożyczonych rowerów (użytkownicy zarejsetrowani, użytkownicy niezarejestrowani, użytkownicy łącznie).W szczególności regresję można badać w zakresie łącznej liczby wypożyczeń. Zbiór wydaje się dość ciekawy do zbadania oraz jest wystarczająco duży.

```{r}
bikes <- read.csv("https://query.data.world/s/jcpgkqdal2ztirvoahx5dmopkpzr7q", header=TRUE, stringsAsFactors=FALSE)
bikes$Casual.Users <- NULL
bikes$Registered.Users <- NULL

```


### Zbiór danych używanych samochodów
Dane pochodzą ze strony: [https://www.kaggle.com/c/usedcarvaluation]()

Zbiór zawiera 70000 obserwacji (przykładów) dotyczących cen sprzedaży samochodów osobowych o określonej marce, wieku, pojemności silnika, mocy, przejechanej odlegółości oraz liczby poprzednich właścicieli. Wszystkie atrybuty poza marką są atrybutami ciągłymi (bądź dającymi się uciąglić). Atrybutem przywidywanym dla tego zbioru jest wartość samochodu.

```{r}
cars <- read.csv("cars.csv", header=TRUE, stringsAsFactors=FALSE)
cars$year <- NULL
```


### Zbiór danych nieruchomości
Dane pochodzą ze strony: [https://www.kaggle.com/quantbruce/real-estate-price-prediction]()

Zbiór zawiera 414 obserwacji dotyczących sprzedaży nieruchomości (domów i mieszkań) opisanych z użyciem daty transakcji, wieku nieruchomości, odległości od przystanku, liczby skelpów w okolicy, położenia geograficznego oraz ceny za jednostkę powierzchni. W przypadku tych danych będziemy przewidywać cenę za jednostkę powierzchni.

```{r}
real_estate <- read.csv("real_estate.csv", header=TRUE, stringsAsFactors=FALSE)
real_estate$No <- NULL
```

## Implementacja

### Normalizacja i denormalizacja danych
W celu poprawnego zastosowania metody kNN (k najbliższych sąsiadów) do ogreślenia zbioru terningowego dane muszą być uprzednio znormalizowane. Jest to wymagane, ponieważ algorytm kNN bazuje na funkcji odległości. Brak normalizacji sprawiłby, że jeden z atrybutów byłby wyróżniony ze względu na większy zakres wartosci. Stosuje się trzy rozdzaje normalizacji:
 - Liniowe przeskalowanie wartości do przedizału $[0-1]$ dla atrybutów ograniczonych (o ustalonych wartościach minimalnej i maksymalnej)
 - Przeskalowanie z użyciem rozkładu normalnego o średniej $Ex = 0$ i wariancji $\sigma^{2} = 1$. - dla wartości nieograniczonych
 - Przydzielenie wartości $[0-1]$ rozłożonych równomiernie dla atrybutów dyskretnych
 
W pakiecie taka normalizacja może zostać wykonana automatycznie przez zastosowanie funckcji ``.

```{r}
source('normalizeChrVec.r')
source('normalizeDataFrame.r')
cars_norm <- normalizeDataFrame(cars, test)
```

### Funkcja kNN (k najbliższych sąsiadów)

Funkcja `getKnn` pozwala na wybranie spośród wskazanego zbioru 

### Model regresji liniowej

### Model drzew regresji

## Użycie pakietu

## Metody regresji lokalnej z pakietu R
### LOESS
### LOWESS

## Wyniki i porównanie z metodami regresji lokalnej dostępnymi w pakiecie R
